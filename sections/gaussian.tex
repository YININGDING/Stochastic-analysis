\newpage
\section{Normal/Gaussian distribution}
\subsection{Gaussian distribution}
$X\sim N(\mu, \sigma^2), X(\omega)\in\R$ if the density is given by 
\begin{equation*}
    f(x) = \frac{1}{(2\pi \det C)^{\frac{n}{2}}} exp\left\{-\frac{(x-\mu)^TC^{-1}(x-\mu)}{2} \right\}
\end{equation*}
with characteristic function $\varphi_X(\lambda) = e^{i\lambda^T \mu - \frac{1}{2}\lambda^T C \lambda}, \quad \lambda \in \R^d.$

If $X$ is random vector such that, $X\sim N(m, C), C_{i,j} = \E(X_i - \E(X_i))(X_j - \E(X_j)).$ 

\subsection{Gaussian process}
\begin{dfn}
A process $X_t$ is \textbf{Gaussian} if for any choice of $t_1, t_2, ..., t_n, n = 1,2,...$
\begin{equation*}
    X^n = (X_{t_1}, ..., X_{t_n})
\end{equation*}  is a Gaussian vector or equivalently $\lambda^T = (\lambda_1, ..., \lambda_n)$
\begin{equation*}
    \lambda^T X_n = \bigs{k=1}^n \lambda_k X_{t_k}
\end{equation*}is a real-valued Gaussian random variable.
\end{dfn}
\begin{thm}
If $X, Y$ are not fully dependent (linear relationship) then the linear combinations still forms Gaussian random variable.
\end{thm}

\begin{thm}
Let $(X, Y_1, ..., Y_n)$ be a Gaussian random vector $\setg =\sigma(Y_1, ..., Y_n)$ s.t. $Y_1, ..., Y_n$ are independent then
\begin{equation*}
    \E(X|\setg) = \E(X) + \bigs{i=1}^n \frac{Cov(X, Y_i)}{Var(Y_i)} (Y_i - \E(Y_i)).
\end{equation*}
\end{thm}

\begin{thm}
Let $X_t$ be Gaussian process then
\begin{equation*}
    \E\int_0^T \abs{X_t}^2 \diff t < \infty \quad \text{iff} \quad \prob(\int_0^T \abs{X_t}^2 \diff t < \infty) = 1.
\end{equation*}
\end{thm}
This will be an useful condition later in Girsanov theorem.